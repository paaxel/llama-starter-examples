{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad7ec54e-589f-4f5f-880d-5f4b18d0bceb",
   "metadata": {},
   "source": [
    "# Hello, Llama: Simple RAG  \n",
    "\n",
    "This notebook provides a comprehensive guide to implementing a simple Retrieval-Augmented Generation (RAG) system using a fantasy animals FAQ dataset. \n",
    "\n",
    "A **RAG (Retrieval-Augmented Generation)** model is a type of machine learning architecture that combines the strengths of two key components:\n",
    "- Retrieval: This involves searching a large corpus of documents (or knowledge base) to find relevant information based on a given query.\n",
    "- Generation: This is where a language model, such as GPT, takes the retrieved information and generates a coherent and contextually appropriate response.\n",
    "\n",
    "### A RAG system in general works by:\n",
    "1. Storing documents in a vector database\n",
    "2. Retrieving relevant information when a query is received\n",
    "3. Using this retrieved context to generate accurate responses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bbf50e-0974-4d82-bf6c-5fab22e54d53",
   "metadata": {},
   "source": [
    "#### Install required libraries\n",
    "\n",
    "Run the following cell to install all necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "371f9314-05fc-48ed-be73-7619909845fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to install required libraries\n",
    "\n",
    "# !pip install langchain==0.3.4\n",
    "# !pip install langchain_community==0.3.3\n",
    "# !pip install langchain_huggingface==0.1.1\n",
    "\n",
    "# Install FAISS. For CUDA supported GPU: `pip install faiss-gpu` | For CPU `pip install faiss-cpu`\n",
    "# !pip install faiss-cpu==1.9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540e1ed0-a7ed-4a4f-adef-bcff17ca8db8",
   "metadata": {},
   "source": [
    "#### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59a6f18d-bce3-40d4-8def-3200352fa6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ccc53c-c4e8-41a5-8206-462edc36fbf7",
   "metadata": {},
   "source": [
    "#### Load dataset in memory\n",
    "\n",
    "We'll load our fantasy animals FAQ from a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f3d838b-048b-45c5-9996-3e258dd0b6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question loaded: 33\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fantasy_animals_file = \"fantasy-animals-faqs.json\"\n",
    "\n",
    "with open(fantasy_animals_file, 'r') as file:\n",
    "    data = json.load(file)  # Load JSON data into a Python dictionary\n",
    "\n",
    "\n",
    "print(f\"Question loaded: {len(data['questions'])}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32878a8b-3489-42d9-926a-871441f5b604",
   "metadata": {},
   "source": [
    "#### Dataset preparation\n",
    "\n",
    "To load documents you can use the prebuilt document loaders you can find here: https://python.langchain.com/docs/integrations/document_loaders/. \n",
    "In this example i will use a manual approach but you can load document with one of these loaders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9aeb2879-6113-4bd3-96bb-726702cba2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents loaded: 33\n",
      "Preview of first document:\n",
      "A Drakelion is a large, lion-like creature with dragon-like scales covering its body. It is known for its fearsome roar and powerful leaps.\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "\n",
    "for question in data['questions']:\n",
    "    documents.append(\n",
    "        Document(\n",
    "            page_content = question['answer'],\n",
    "            metadata = {'title': question['title']}\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Display basic information about loaded documents\n",
    "print(f\"Number of documents loaded: {len(documents)}\")\n",
    "print(f\"Preview of first document:\\n{documents[0].page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ec82e7-1901-4ff9-a652-7f9a687a3b3e",
   "metadata": {},
   "source": [
    "### Splitting document to optimize retrival\n",
    "\n",
    "When building a RAG (Retrieval-Augmented Generation) system, it's crucial to break the content into smaller pieces to ensure the context provided to the LLM (Large Language Model) isn't too large. This is important because many paid LLM services charge based on the number of tokens in each request. If the content isn't properly split, you may incur higher costs and, in some cases, the context size may exceed the LLM's processing limit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac980dc-6b2f-4b2b-8b23-144a4e4c084f",
   "metadata": {},
   "source": [
    "#### Document Splitting Strategies\n",
    "\n",
    "##### Chunk-based Splitting\n",
    "Chunk-based splitting is a straightforward method but comes with notable drawbacks. It often splits text arbitrarily, potentially breaking sentences or words, leading to a loss of context and reduced comprehensibility. Furthermore, if the chosen separator is absent or inconsistently applied, chunk sizes can vary significantly, resulting in processing challenges. These chunks may also begin or end abruptly, diminishing readability and overall coherence.\n",
    "  \n",
    "##### Semantic Text Splitting\n",
    "A more advanced technique is semantic text splitting, which includes methods like sentence-based and paragraph-based splitting:\n",
    "- **Sentence-based splitting** uses natural language processing (NLP) to divide the text into complete sentences, preserving context and improving readability.\n",
    "- **Paragraph-based splitting**, on the other hand, divides the text into paragraphs, which is especially useful for longer documents, as it maintains the overall structure and coherence of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e713fd63-ce10-4745-aaf1-7b577e978f64",
   "metadata": {},
   "source": [
    "#### CharacterTextSplitter\n",
    "\n",
    "In this method, text is split using a basic approach that relies on chunk size and a separator.\n",
    "\n",
    "Parameters:\n",
    "- `separator`, specifies the character or sequence of characters at which the text should be split. It defines the boundary where the text will be divided into chunks.\n",
    "- `chunk_size`, sets the maximum size (in characters) for each chunk of text. It determines how large each individual chunk can be.\n",
    "- `chunk_overlap`, defines the number of characters that should overlap between consecutive chunks. It allows for some characters to be shared between chunks, which can be useful for maintaining context across chunks.\n",
    "\n",
    "**Important:**   \n",
    "If no separator is found in the text, the chunk could indeed be larger than the specified `chunk_size`. The `CharacterTextSplitter` will attempt to split the text at the specified separator (`\\n\\n` in this case). If the separator is not found within a chunk of text, the splitter may not be able to enforce the `chunk_size` limit strictly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b8d4375-aba8-49c9-b29c-73bd4425757f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    separator='\\n\\n', # split at every double newline\n",
    "    chunk_size=200, # maximum size of each chunk can contain up to 200 characters\n",
    "    chunk_overlap=0 # no overlap\n",
    ")\n",
    "\n",
    "splitted_documents = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a75b9c3-fc11-4091-831a-776f51fe300a",
   "metadata": {},
   "source": [
    "In our specific case, the documents will not be split because the text does not contain double line breaks (\\n\\n). Since the CharacterTextSplitter relies on the presence of the specified separator to divide the text, its absence prevents the text from being split, potentially resulting in a single, large chunk that exceeds the intended chunk_size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6ffd63-70f2-4447-b1c3-9d55713dcbdc",
   "metadata": {},
   "source": [
    "### Loading Documents into a VectoreStore\n",
    "\n",
    "In this example, we use FAISS (Facebook AI Similarity Search) as the vector store. A vector store is a specialized data structure designed to store and efficiently manage high-dimensional vectors. FAISS, developed by Facebook AI, is a library optimized for similarity search and clustering of dense vectors. You can find more details about FAISS [here](https://python.langchain.com/docs/integrations/vectorstores/faiss/).\n",
    "\n",
    "To store documents in a vector store, the text must first be converted into vector representations. This is achieved using **SentenceTransformer** from the [sentence-transformers](https://huggingface.co/sentence-transformers) library.\n",
    "\n",
    "\n",
    "#### What we will do:\n",
    "- **Generate Embeddings**: using SentenceTransformer we generate dense vector embeddings for sentences or text snippets. These embeddings capture the semantic meaning of the text, which is critical for applications like semantic search or sentence similarity.\n",
    "- **Store in Vector Store**: The generated embeddings are stored in FAISS for efficient retrieval based on similarity to a given query.\n",
    "\n",
    "This combination of SentenceTransformer and FAISS enables semantic search, allowing us to retrieve and compare text based on its meaning rather than relying solely on keyword matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aeebc753-b397-4356-98e4-44899bba7c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"FILL_WITH_BASE_FOLDER\" # Example: \"C:/Users/username/Documents/HuggingFace\"\n",
    "\n",
    "# we will use the model https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "# set the model id\n",
    "model_id = os.path.join(base_folder, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcde6f0-b51b-49aa-9529-d0b027a496b6",
   "metadata": {},
   "source": [
    "### Initialize embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55e77412-d60f-4d29-b82f-44b51e1bf194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HuggingFace embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57441527-f6c9-4be7-978c-d4cbf32d298c",
   "metadata": {},
   "source": [
    "The `HuggingFaceEmbeddings` class loads a pre-trained model (specified by `model_id`) from the HuggingFace library to generate embeddings for your documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906fb467-1d64-4092-9163-12b187daa16a",
   "metadata": {},
   "source": [
    "### Initialize Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "277157f2-cb19-475a-ae46-13c21559bd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_documents(splitted_documents, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a270615-352c-4fc4-9d1b-b03131cd1b0a",
   "metadata": {},
   "source": [
    "Here, the FAISS vector store is initialized by converting the pre-processed `splitted_documents` into vector representations using the specified `embedding_model`. These vectors are then stored in the FAISS database, enabling efficient similarity searches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbef1924-d374-476e-8f06-2d74c6a90876",
   "metadata": {},
   "source": [
    "### Execute similarity search and enrich prompt\n",
    "\n",
    "Once the vector store is ready, we can perform a similarity search.\n",
    "\n",
    "#### Basic Workflow:\n",
    "- **Input Query**: Provide a query or question.\n",
    "- **Retrieve Similar Documents**: The similarity search identifies documents most similar to the query, based on their embeddings in the vector space.\n",
    "- **Enhance the Prompt**: Add the retrieved documents to the prompt to provide additional context for the LLM (Large Language Model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fdaad0f-bfa3-4feb-85ae-1530d7d7e716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document similar with order 0\n",
      "- Title: What do Drakelions eat?\n",
      "- Page Content: Drakelions are carnivores, feeding primarily on large mammals such as mountain deer, wild boars, and sometimes even smaller dragons.\n",
      "--------------------------------------\n",
      "\n",
      "Document similar with order 1\n",
      "- Title: What is a Drakelion?\n",
      "- Page Content: A Drakelion is a large, lion-like creature with dragon-like scales covering its body. It is known for its fearsome roar and powerful leaps.\n",
      "--------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Which is the food of the Drakelion?\"\n",
    "\n",
    "number_of_similar_documents_to_find = 2\n",
    "\n",
    "results = vector_store.similarity_search(query, k=number_of_similar_documents_to_find)\n",
    "\n",
    "for i in range(0, len(results)):\n",
    "    print(f\"Document similar with order {i}\")\n",
    "    print(f\"- Title: {results[i].metadata['title']}\")\n",
    "    print(f\"- Page Content: {results[i].page_content}\")\n",
    "    print(\"--------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2523de6d-32f4-4508-8fed-74b3ffa8de19",
   "metadata": {},
   "source": [
    "#### Retrieve the most Similar Document\n",
    "\n",
    "While multiple documents can generally be passed to an LLM for generating responses, certain models, like the simplified Llama model used here, benefit from a more focused approach to reduce the risk of hallucinations. Therefore, only the most relevant document will be selected and added to the prompt in this Retrieval-Augmented Generation (RAG) process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3528bbb4-0318-4382-80dd-f6e9f4046ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Drakelions are carnivores, feeding primarily on large mammals such as mountain deer, wild boars, and sometimes even smaller dragons.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = results[0].page_content\n",
    "\n",
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ef0254-477e-480f-9a31-74fa8f88993b",
   "metadata": {},
   "source": [
    "#### Build HuggingFace pipeline to be used in langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "373524f2-b4be-474d-8f53-f3f8e2f320e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36a827a713ba4445bdf4b56824d52b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_folder = \"FILL_WITH_BASE_FOLDER\" # Example: \"C:/Users/username/Documents/HuggingFace\"\n",
    "\n",
    "model_name = \"Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# set the model id\n",
    "model_id = os.path.join(base_folder, model_name)\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id\n",
    ")\n",
    "\n",
    "pipe = transformers.pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_new_tokens=128, \n",
    "    top_k=50, \n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "\n",
    "hf_pipeline = HuggingFacePipeline(\n",
    "    pipeline=pipe\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621e7192-7ca6-4e9f-8fe0-9a8f31f31611",
   "metadata": {},
   "source": [
    "#### Define prompt to be used in langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3c9ec9a-e7ff-45de-8b74-1ec7ff61554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "System: You are an expert in fantasy animals. To answer consider the 'Extra Information' provided. If you don't know the answer respond with \"I don't know the answer.\" without giving any explanation. \n",
    "\n",
    "Extra information: {context}\\n\\n\n",
    "\n",
    "Query: {query}\n",
    "Response:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "\n",
    "# chaining prompt_template and pipeline\n",
    "chain = prompt_template | hf_pipeline.bind(skip_prompt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2d009c-e690-4cda-8f11-2a3ca7d3cc1e",
   "metadata": {},
   "source": [
    "#### Execute the call to the RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66aca824-3043-4af2-89df-511a95bb514e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The query is: 'Which is the food of the Drakelion?'\n",
      "The context is: 'Drakelions are carnivores, feeding primarily on large mammals such as mountain deer, wild boars, and sometimes even smaller dragons.'\n",
      "\n",
      "The Response is: \n",
      "'Drakelions feed primarily on large mammals such as mountain deer, wild boars, and sometimes even smaller dragons.'\n"
     ]
    }
   ],
   "source": [
    "# example of unkown capital\n",
    "input_dict = {\"query\": query, \"context\": context}\n",
    "\n",
    "result = chain.invoke(input_dict)\n",
    "\n",
    "print(f\"\\nThe query is: '{query}'\")\n",
    "print(f\"The context is: '{context}'\")\n",
    "print(f\"\\nThe Response is: \\n'{result}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dab92cd-d24a-4bfe-9c1b-ab8199f3b902",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "In this example, we demonstrated how to create a simple Retrieval-Augmented Generation (RAG) system using an in-memory vector store. By integrating a HuggingFace model with LangChain, we were able to efficiently retrieve relevant context and generate accurate responses based on that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc566dd-8e7b-402a-b757-c1f08c722439",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
