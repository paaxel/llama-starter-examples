{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad7ec54e-589f-4f5f-880d-5f4b18d0bceb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Hello, Llama: Simple RAG using QDrant  \n",
    "\n",
    "This notebook provides a guide to implementing a simple Retrieval-Augmented Generation (RAG) system using a fantasy animals FAQ dataset ingested in QDRant. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91641209-baa9-44d8-825d-900759ad05e8",
   "metadata": {},
   "source": [
    "# Install QDrant on Docker or Create a Cloud Subscription\n",
    "\n",
    "The first step is the setup of a QDrant cluster.  \n",
    "To do this you can:\n",
    "- Set up a Qdrant cluster using Docker, **or**\n",
    "- Create a free cluster on [Qdrant Cloud](https://cloud.qdrant.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757cfe6e-e375-4d7f-9dc2-a8a98f193edd",
   "metadata": {},
   "source": [
    "## Creating Clusters on https://cloud.qdrant.io/\n",
    "\n",
    "The simplest method is to create a free account on [https://cloud.qdrant.io/](https://cloud.qdrant.io/) and then create a cluster. In this example this approach will used.\n",
    "\n",
    "<img src=\"./images/create-cluster.png\" width=\"600\">\n",
    "After clicking \"create\", it's important to copy the generated authentication token. Save the API key in a secure place.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "Now you can copy the Qdrant cluster endpoint to connect later:\n",
    "<img src=\"./images/cluster-endpoint.png\" width=\"600\">\n",
    "\n",
    "<br>\n",
    "You can open the dashboard by clicking the following button:\n",
    "<img src=\"./images/cluster-dashboard.png\" width=\"600\">\n",
    "\n",
    "<br>\n",
    "And then navigate through the created collections, which will initially be empty:\n",
    "<img src=\"./images/collections.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1caf7e-50c2-4319-947d-dafca3de7e64",
   "metadata": {},
   "source": [
    "### Installazione di QDrant su Docker\n",
    "\n",
    "To install QDrant using Docker, you'll first need to install Docker (follow the instructions at https://docs.docker.com/get-started/get-docker/) and Docker Compose.\n",
    "\n",
    "Use the following docker-compose.yaml file. Replace the placeholder `<ADD_HERE_AN_API_KEY>` with a random alphanumeric string of at least 25 characters.\n",
    "\n",
    "```yaml\n",
    "services:\n",
    "  qdrant:\n",
    "    image: qdrant/qdrant\n",
    "    container_name: qdrant\n",
    "    ports:\n",
    "      - \"127.0.0.1:6333:6333\"\n",
    "    environment:\n",
    "      - QDRANT__SERVICE__API_KEY=<ADD_HERE_AN_API_KEY>\n",
    "    volumes:\n",
    "      - qdrant_storage:/qdrant/storage\n",
    "  \n",
    "\n",
    "volumes:\n",
    "  qdrant_storage:\n",
    "```\n",
    "\n",
    "Once you've created the docker-compose.yaml file with the above content, run the following command:\n",
    "\n",
    "> `docker-compose up`\n",
    "\n",
    "At this point, you can access QDrant by navigating to http://127.0.0.1:6333/dashboard in your browser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665313e1-6b31-494c-83fa-13fb8b3cf442",
   "metadata": {},
   "source": [
    "### Configure Qdrant Connection Settings\n",
    "\n",
    "Now we'll set up the essential variables needed to connect to Qdrant - specifically the API key and cluster URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f35f09c-cd66-4241-9892-5f1c4c70b86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_API_KEY = '<QDRANT_API_KEY>'\n",
    "QDRANT_CLUSTER_URL = '<QDRANT_CLUSTER_URL>'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bbf50e-0974-4d82-bf6c-5fab22e54d53",
   "metadata": {},
   "source": [
    "### Install required libraries\n",
    "\n",
    "Run the following cell to install all required packages (this includes only additional libraries not installed in previous examples. It's recommended to follow them in the specified order):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371f9314-05fc-48ed-be73-7619909845fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to install required libraries\n",
    "\n",
    "#!pip install langchain==0.3.4\n",
    "#!pip install langchain_community==0.3.3\n",
    "#!pip install langchain_huggingface==0.1.1\n",
    "\n",
    "# install qdrant-client\n",
    "\n",
    "#!pip install qdrant-client==1.13.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540e1ed0-a7ed-4a4f-adef-bcff17ca8db8",
   "metadata": {},
   "source": [
    "#### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a6f18d-bce3-40d4-8def-3200352fa6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import PointStruct\n",
    "from qdrant_client.http.exceptions import UnexpectedResponse\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ccc53c-c4e8-41a5-8206-462edc36fbf7",
   "metadata": {},
   "source": [
    "#### Load dataset in memory\n",
    "\n",
    "We'll load our fantasy animals FAQ from a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3d838b-048b-45c5-9996-3e258dd0b6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fantasy_animals_file = \"fantasy-animals-faqs.json\"\n",
    "qdrant_collection_name = 'fantasy_animals_faqs'\n",
    "\n",
    "with open(fantasy_animals_file, 'r') as file:\n",
    "    data = json.load(file)  # Load JSON data into a Python dictionary\n",
    "\n",
    "\n",
    "print(f\"Question loaded: {len(data['questions'])}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32e12b0-c414-4244-b550-8e72c4adf4f6",
   "metadata": {},
   "source": [
    "### Create Qdrant Collection Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cc93aa-38aa-4790-927e-7a5469c6a07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qdrant_collection(qdrant_client: QdrantClient, collection_name:str, vectors_config):\n",
    "    try:\n",
    "        created = qdrant_client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=vectors_config\n",
    "        )\n",
    "        \n",
    "        if created:\n",
    "            print(f\"Collection '{collection_name}' created successfully.\")\n",
    "        else:\n",
    "            print(f\"Failed to create collection '{collection_name}'.\")\n",
    "    except UnexpectedResponse as e:\n",
    "        if e.status_code == 409:\n",
    "            print(f\"Collection '{collection_name}' already exists.\")\n",
    "        else:\n",
    "            print(f\"Unexpected error creating collection '{collection_name}': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dba636-9a53-42a1-bc60-8c6192fc721a",
   "metadata": {},
   "source": [
    "### Import data on Qdrant Collection Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3512e548-fd48-4fb5-86ce-883d5919e670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_on_qdrant(collection_name: str, texts: [str], model_embedder: SentenceTransformer, qdrant_client: QdrantClient):\n",
    "    \n",
    "    embeddings = model_embedder.encode(texts)\n",
    "    \n",
    "    # Prepare points for ingestion\n",
    "    points = []\n",
    "\n",
    "    collection_info = qdrant_client.get_collection(collection_name)\n",
    "    offset = collection_info.points_count\n",
    "\n",
    "    for i, (embedding, text) in enumerate(zip(embeddings, texts)):\n",
    "        \n",
    "        point_struct = PointStruct(id=offset+i, vector=embedding.tolist(), payload={\"text\": text})\n",
    "        \n",
    "        points.append(point_struct)\n",
    "    \n",
    "    \n",
    "    qdrant_client.upsert(collection_name=collection_name, points=points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790be8b6-262e-4f07-b7b4-cbbaf202b7b0",
   "metadata": {},
   "source": [
    "### Query Utility Method for Qdrant Collection Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb71fe0-95ef-4eff-b3b6-88ce236fc411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query(query_text: str, collection_name: str,  model_embedder: SentenceTransformer, qdrant_client: QdrantClient, max_number_of_res_points: int = 3):\n",
    "    # compute the embedding of the query text\n",
    "    query_vector = model_embedder.encode([query_text])[0]\n",
    "    \n",
    "    # Search for similar points\n",
    "    search_result = qdrant_client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_vector,\n",
    "        limit=max_number_of_res_points\n",
    "    )\n",
    "\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0695c18a-c9bc-4179-9e94-cf84a053bdeb",
   "metadata": {},
   "source": [
    "### Create Qdrant Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21e20b6-c38e-4f5f-83b5-6acc1380ca12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Qdrant\n",
    "client_qd = QdrantClient(url=QDRANT_CLUSTER_URL, api_key=QDRANT_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d330951d-02f4-4d7f-8f14-d5193731e3c9",
   "metadata": {},
   "source": [
    "### Create embedder model\n",
    "\n",
    "Creare l'embedder model che è il model che verrà utilizzato per calcolare l'embedding dei documenti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeebc753-b397-4356-98e4-44899bba7c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"FILL_WITH_BASE_FOLDER\" # Example: \"C:/Users/username/Documents/HuggingFace\"\n",
    "\n",
    "# we will use the model https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "# set the model id\n",
    "model_id = os.path.join(base_folder, model_name)\n",
    "\n",
    "# Initialize the embedder\n",
    "model_st = SentenceTransformer(model_id, device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299a3ab5-4daf-4f99-8fc9-d89706445e45",
   "metadata": {},
   "source": [
    "#### Check how the embedder works\n",
    "\n",
    "This step also sets the EMBEDDING_MODEL_SIZE that will be the size of our embedding vectors on Qdrant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb3392d-fdb3-47cf-aa45-3b1ca2f9aa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_example = model_st.encode(\"A Drakelion is a large, lion-like creature with dragon-like scales covering its body.\")\n",
    "\n",
    "subset_num = 50\n",
    "print(f\"First {subset_num} array elements {embedding_example[0:subset_num]}\\n\")\n",
    "print(f\"Embedding size: {embedding_example.shape[0]}\")\n",
    "\n",
    "EMBEDDING_MODEL_SIZE = embedding_example.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376f4345-55a3-4956-b1a9-e4fdb32ee103",
   "metadata": {},
   "source": [
    "## RAG Creation\n",
    "\n",
    "To create the RAG we need to execute the following steps:\n",
    "1. **Collection Setup** - Create a Qdrant collection with the correct vector dimensions and similarity metric\n",
    "2. **Data Ingestion** - Process documents into chunks, generate embeddings, and index them in Qdrant\n",
    "3. **Query Testing** - Validate retrieval performance with sample queries and inspect results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b91fc50-947e-4030-ade4-085a35d72b84",
   "metadata": {},
   "source": [
    "#### Collection setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ddf0e8-8c43-4a0b-8ceb-632ee9d86938",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_config={\"size\": EMBEDDING_MODEL_SIZE, \"distance\": \"Cosine\"}\n",
    "\n",
    "# call utility method to create collection\n",
    "create_qdrant_collection(client_qd, qdrant_collection_name, vectors_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0ad77a-4fc1-4ca5-b48e-f38f1555df9f",
   "metadata": {},
   "source": [
    "#### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c9d209-adc5-47b3-8462-c37d5aa145ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in tqdm(data['questions'], desc=\"FAQs ingestion\"):\n",
    "    \n",
    "    text = f\"Title: {elem['title']}, Answer: {elem['answer']}\"\n",
    "    \n",
    "    import_on_qdrant(qdrant_collection_name, [text], model_st, client_qd)\n",
    "\n",
    "print(\"Ingestion completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd97c52-6b56-4536-9217-e9bb87a3e286",
   "metadata": {},
   "source": [
    "#### Verify Data Ingestion\n",
    "\n",
    "After completing the ingestion process, you can confirm the data was properly indexed in your Qdrant collection:\n",
    "\n",
    "<img src=\"images/collection-created.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc73bfe-accf-42f3-a6d4-e1b4f7ed0344",
   "metadata": {},
   "source": [
    "### Quering Data\n",
    "\n",
    "We'll implement semantic search to retrieve the most relevant context from our vector database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bef0f8-1fe7-43dd-bd6c-76278c92b2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"Which is the food of the Drakelion?\"\n",
    "\n",
    "points = execute_query(query_text, qdrant_collection_name, model_st, client_qd, 3)\n",
    "\n",
    "print('Results:\\n')\n",
    "\n",
    "for point in points:\n",
    "    print(f\"ID: {point.id}\\nScore: {point.score}\\nPayload: {point.payload}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90305ba9-c7c0-4dc7-8c1c-93ce7c2eb8f6",
   "metadata": {},
   "source": [
    "#### Build HuggingFace pipeline to be used in langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6215b1b1-f31d-4b60-b720-ddd689bbef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = \"FILL_WITH_BASE_FOLDER\" # Example: \"C:/Users/username/Documents/HuggingFace\"\n",
    "\n",
    "model_name = \"Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# set the model id\n",
    "model_id = os.path.join(base_folder, model_name)\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id\n",
    ")\n",
    "\n",
    "pipe = transformers.pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_new_tokens=128, \n",
    "    top_k=50, \n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "\n",
    "hf_pipeline = HuggingFacePipeline(\n",
    "    pipeline=pipe\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b6e1b1-c81e-4d52-a2f7-f0b19900d296",
   "metadata": {},
   "source": [
    "#### Define prompt to be used in langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452eb558-e50e-4726-81e1-a60f2e2772ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "System: You are an expert in fantasy animals. To answer consider the 'Extra Information' provided. If you don't know the answer respond with \"I don't know the answer.\" without giving any explanation. \n",
    "\n",
    "Extra information: {context}\\n\\n\n",
    "\n",
    "Query: {query}\n",
    "Response:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "\n",
    "# chaining prompt_template and pipeline\n",
    "chain = prompt_template | hf_pipeline.bind(skip_prompt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c407b4-d398-44b6-8437-aaee7d06db4e",
   "metadata": {},
   "source": [
    "#### Execute the call to the RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4e72dd-fa5d-4535-8c9d-ccd70f009d93",
   "metadata": {},
   "source": [
    "##### Retrieve the context to be used in the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0813e4-a426-4ee9-b6a0-d5b5b5883426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use Qdrant to make semantic search to retrieve the most relevant context from our vector database:\n",
    "\n",
    "context = points[0].payload['text']\n",
    "\n",
    "print(f\"Most probable point for '{query_text}'\\n\\n{context}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e577854c-3bf3-4667-9cdd-f667d3a4f2b9",
   "metadata": {},
   "source": [
    "##### Execute the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eece5698-d5c9-44a7-846a-88a08e4c31bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dict = {\"query\": query_text, \"context\": context}\n",
    "\n",
    "result = chain.invoke(input_dict)\n",
    "\n",
    "print(f\"\\nThe query is: '{query_text}'\")\n",
    "print(f\"The context is: '{context}'\")\n",
    "print(f\"\\nThe Response is: \\n'{result}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bf37d1-47e8-49c7-949b-7e10783f82bd",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "This implementation demonstrates how to build a basic RAG system using QDrant for vector search and a Llama model for generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ada143-ac02-4832-8021-9921b04c43fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
